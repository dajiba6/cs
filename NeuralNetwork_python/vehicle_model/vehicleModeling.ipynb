{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===================================== Config =====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "input size:2, ouput size:1, hidden size:8, num layers:2\n",
      "batch size:20, learning rate:0.002, epochs:100, sequence length: 25\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "================================ Data processing =================================\n",
      "=============================== Using device: cuda ===============================\n",
      "/home/cyn/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original samples: 307509\n",
      "Total filtered samples: 36130\n",
      "throttle left:41578, brake left:0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cyn/miniconda3/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.005466418869229184\n",
      "Epoch [2/100], Loss: 0.001954828782317496\n",
      "Epoch [3/100], Loss: 0.0019606270600143407\n",
      "Epoch [4/100], Loss: 0.0019572745220133364\n",
      "Epoch [5/100], Loss: 0.0019450801604974492\n",
      "Epoch [6/100], Loss: 0.0019327448044642527\n",
      "Epoch [7/100], Loss: 0.0019337318197551673\n",
      "Epoch [8/100], Loss: 0.001935187674763197\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 396\u001b[0m\n\u001b[1;32m    391\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# visualize_data(dataset)\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[38;5;66;03m# # 训练\u001b[39;00m\n\u001b[0;32m--> 396\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# # 测试\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;66;03m# test_loader = DataLoader(val_set, batch_size=batch_size)\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;66;03m# test(model, test_loader, criterion)\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# 保存\u001b[39;00m\n\u001b[1;32m    403\u001b[0m current_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mlocaltime())\n",
      "Cell \u001b[0;32mIn[5], line 272\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m    271\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[5], line 193\u001b[0m, in \u001b[0;36mRecordDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 193\u001b[0m         features \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[idx]\u001b[38;5;241m.\u001b[39miloc[:, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m    199\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import threading\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import firwin\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "print(torch.version.cuda)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "\n",
    "def interpolate_outliers(data_samples, threshold=3):\n",
    "    outlier_indices = []  # 存储异常值的行号列表\n",
    "    for feature in data_samples:\n",
    "        feature_mean = data_samples[feature].mean()\n",
    "        feature_std = data_samples[feature].std()\n",
    "\n",
    "        standardized_residuals = (data_samples[feature] - feature_mean) / feature_std\n",
    "        outliers_mask = abs(standardized_residuals) > threshold\n",
    "        outliers_indices = data_samples[outliers_mask].index\n",
    "\n",
    "        # 将当前特征的异常值索引添加到异常值行号列表中\n",
    "        outlier_indices.extend(outliers_indices)\n",
    "\n",
    "    return outlier_indices\n",
    "\n",
    "\n",
    "def print_aligned_title(title):\n",
    "    total_length = 80\n",
    "    title_length = len(title)\n",
    "    left_padding_length = (total_length - title_length) // 2\n",
    "    right_padding_length = total_length - title_length - left_padding_length\n",
    "    aligned_title = (\n",
    "        left_padding_length * \"=\" + \" \" + title + \" \" + right_padding_length * \"=\"\n",
    "    )\n",
    "    logging.info(aligned_title)\n",
    "\n",
    "\n",
    "def moving_average_filter(data, window_size):\n",
    "    \"\"\"\n",
    "    滑动窗口滤波\n",
    "    \"\"\"\n",
    "    filled_data = data.rolling(window=window_size, min_periods=1).mean()\n",
    "    return filled_data\n",
    "\n",
    "\n",
    "def fir_filter(data, num_taps, cutoff_freq, window=\"hamming\"):\n",
    "    \"\"\"\n",
    "    有限冲激响应(FIR)滤波器\n",
    "    参数:\n",
    "        num_taps (int): 滤波器阶数（滤波器长度）。\n",
    "        cutoff_freq (float): 滤波器的截止频率。\n",
    "        window (str, optional): 要使用的窗口函数类型。默认为 'hamming'。\n",
    "    \"\"\"\n",
    "    filter_coeffs = firwin(num_taps, cutoff=cutoff_freq, window=window)\n",
    "    filtered_data = np.convolve(data, filter_coeffs, mode=\"same\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# 数据集\n",
    "class RecordDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length):\n",
    "        self.mode = 1  # 1 throttle; 2 brake\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data_samples = self.data[\n",
    "            [\n",
    "                \"throttle_percentage\",\n",
    "                \"brake_percentage\",\n",
    "                \"speed_mps\",\n",
    "                \"steering_percentage\",\n",
    "                \"acceleration_current_point\",\n",
    "                \"acceleration_next_point\",\n",
    "                \"angular_velocity_vrf\",\n",
    "            ]\n",
    "        ]\n",
    "        self.data_ori = self.data_samples.copy()\n",
    "\n",
    "        # 滤波\n",
    "        for column in self.data_samples.columns:\n",
    "            self.data_samples.loc[:, column] = moving_average_filter(\n",
    "                self.data_samples[column], window_size=3\n",
    "            )\n",
    "        # for column in self.self.data_samples.columns:\n",
    "        #     self.data_samples.loc[:, column] = fir_filter(\n",
    "        #         self.data_samples[column], 3, 1e-20\n",
    "        #     )\n",
    "        self.data_samples_no_extraction = self.data_samples.copy()\n",
    "\n",
    "        # TODO: standardlized residual 去除 outliers\n",
    "        self.sequence_length = sequence_length\n",
    "        self.throttle_count = 0\n",
    "        self.brake_count = 0\n",
    "        big_group = []\n",
    "        index_tokeep = []\n",
    "        normalization_list = [[] for _ in range(len(self.data_samples.columns))]\n",
    "\n",
    "        # 筛选数据\n",
    "        for i in range(len(self.data_samples)):\n",
    "\n",
    "            steering_condition = (\n",
    "                abs(self.data_samples.at[i, \"steering_percentage\"]) <= 1\n",
    "            )\n",
    "            cmd_condition = (\n",
    "                self.data_samples.at[i, \"throttle_percentage\"] >= 1\n",
    "                or self.data_samples.at[i, \"brake_percentage\"] >= 1\n",
    "            )\n",
    "            cmd_throttle = self.data_samples.at[i, \"brake_percentage\"] == 0\n",
    "            cmd_brake = self.data_samples.at[i, \"throttle_percentage\"] == 0\n",
    "            speed_condition = self.data_samples.at[i, \"speed_mps\"] > 0\n",
    "            # 训练油门模型\n",
    "            if (\n",
    "                self.mode == 1\n",
    "                and steering_condition\n",
    "                and cmd_condition\n",
    "                and cmd_throttle\n",
    "                and speed_condition\n",
    "            ):\n",
    "                index_tokeep.append(i)\n",
    "            # 训练刹车模型\n",
    "            elif (\n",
    "                self.mode == 2\n",
    "                and steering_condition\n",
    "                and cmd_condition\n",
    "                and cmd_brake\n",
    "                and speed_condition\n",
    "            ):\n",
    "                index_tokeep.append(i)\n",
    "\n",
    "            else:\n",
    "                if len(index_tokeep) >= self.sequence_length:\n",
    "                    # 存储需要归一化的数据\n",
    "                    for idx in index_tokeep:\n",
    "                        row_data = self.data_samples.iloc[idx]\n",
    "                        for col_idx, col_name in enumerate(self.data_samples.columns):\n",
    "                            normalization_list[col_idx].append(row_data[col_name])\n",
    "\n",
    "                    small_group_data = [\n",
    "                        self.data_samples.iloc[idx] for idx in index_tokeep\n",
    "                    ]\n",
    "                    small_group_df = pd.DataFrame(small_group_data)\n",
    "                    big_group.append(small_group_df)\n",
    "                index_tokeep = []\n",
    "\n",
    "        # 最后一个数据\n",
    "        if len(index_tokeep) >= self.sequence_length:\n",
    "            for idx in index_tokeep:\n",
    "                row_data = self.data_samples.iloc[idx]\n",
    "                for col_idx, col_name in enumerate(\n",
    "                    self.data_samples.columns\n",
    "                ):  # 返回列序号及列名字\n",
    "                    normalization_list[col_idx].append(row_data[col_name])\n",
    "            small_group_data = [self.data_samples.iloc[idx] for idx in index_tokeep]\n",
    "            small_group_df = pd.DataFrame(small_group_data)\n",
    "            big_group.append(small_group_df)\n",
    "\n",
    "        # 归一化\n",
    "        normalization_list = np.array(normalization_list)\n",
    "        self.max_list = np.max(normalization_list, axis=1)\n",
    "        self.min_list = np.min(normalization_list, axis=1)\n",
    "        for group in big_group:\n",
    "            for data_idx, row_data in group.iterrows():\n",
    "                if row_data[\"throttle_percentage\"] > 0:\n",
    "                    self.throttle_count += 1\n",
    "                else:\n",
    "                    self.brake_count += 1\n",
    "                group.loc[data_idx] = (row_data - self.min_list) / (\n",
    "                    self.max_list - self.min_list\n",
    "                )\n",
    "\n",
    "        self.samples = []\n",
    "        for group in big_group:\n",
    "            for i in range(len(group)):\n",
    "                if i + sequence_length <= len(group):\n",
    "                    self.samples.append(group.iloc[i : i + sequence_length])\n",
    "\n",
    "        print_aligned_title(\"Data processing\")\n",
    "        print(f\"Total original samples: {len(self.data_samples)}\")\n",
    "        print(f\"Total filtered samples: {len(self.samples)}\")\n",
    "        print(f\"throttle left:{self.throttle_count}, brake left:{self.brake_count}\\n\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # 选择用于训练的列\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 1:\n",
    "            features = torch.tensor(\n",
    "                self.samples[idx].iloc[:, [0, 2]].values, dtype=torch.float32\n",
    "            )\n",
    "        else:\n",
    "            features = torch.tensor(\n",
    "                self.samples[idx].iloc[:, [1, 2]].values, dtype=torch.float32\n",
    "            )\n",
    "        target = torch.tensor(\n",
    "            self.samples[idx].iloc[-1, 5], dtype=torch.float32\n",
    "        )\n",
    "        return features, target\n",
    "\n",
    "\n",
    "class GenerateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    生成标定表的数据集\n",
    "    参数：\n",
    "      max_list:[0]cmd,[1]speed,[2]acc\n",
    "      min_list:[0]cmd,[1]speed,[2]acc\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sequence_length, max_list, min_list):\n",
    "        self.samples = []\n",
    "        cmd = np.arange(0, 81, 5)\n",
    "        speed = np.arange(0, 10.1, 0.2)\n",
    "        self.cmd, self.speed = np.meshgrid(cmd, speed)\n",
    "        self.input_data_ori = np.column_stack((self.cmd.ravel(), self.speed.ravel()))\n",
    "        input_length = len(self.input_data_ori)\n",
    "        # 归一化\n",
    "        max_cmd = max_list[0]\n",
    "        min_cmd = min_list[0]\n",
    "        max_speed = max_list[1]\n",
    "        min_speed = min_list[1]\n",
    "        normalize_data = self.input_data_ori.copy()\n",
    "        normalize_data[:, 0] = (normalize_data[:, 0] - min_cmd) / (max_cmd - min_cmd)\n",
    "        normalize_data[:, 1] = (normalize_data[:, 1] - min_speed) / (\n",
    "            max_speed - min_speed\n",
    "        )\n",
    "        input_data = np.repeat(normalize_data, repeats=sequence_length, axis=0)\n",
    "        df = pd.DataFrame(input_data, columns=[\"cmd\", \"speed\"])\n",
    "        for i in range(input_length):\n",
    "            self.samples.append(df.iloc[i : i + sequence_length])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # 选择用于训练的列\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(\n",
    "            self.samples[idx].iloc[:, [0, 1]].values, dtype=torch.float32\n",
    "        )\n",
    "        target = torch.tensor(self.samples[idx].iloc[-1, -1], dtype=torch.float32)\n",
    "        return features, target\n",
    "\n",
    "\n",
    "# 网络\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 训练\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    final_loss = 100\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for features, target in train_loader:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        final_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {final_loss}\")\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "# 测试\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for features, target in test_loader:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Test Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "\n",
    "# 数据可视化\n",
    "def visualize_data(data):\n",
    "    for i, feature in enumerate(data.data_samples.columns):\n",
    "        fig, axs = plt.subplots(figsize=(12, 6))\n",
    "        # 原始特征\n",
    "        axs.plot(\n",
    "            data.data_ori.index,\n",
    "            data.data_ori[feature],\n",
    "            label=f\"Original {feature}\",\n",
    "            zorder=1,\n",
    "        )\n",
    "        axs.set_title(f\"{feature} Before and After Smoothing\")  # 设置子图标题\n",
    "        axs.set_xlabel(\"Index\")\n",
    "        axs.set_ylabel(\"Feature Value\")\n",
    "        axs.legend()  # 添加图例\n",
    "\n",
    "        # 平滑后的特征\n",
    "        axs.plot(\n",
    "            data.data_samples_no_extraction.index,\n",
    "            data.data_samples_no_extraction[f\"{feature}\"],\n",
    "            label=f\"Smoothed {feature}\",\n",
    "            zorder=2,\n",
    "        )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_calibration_table(\n",
    "    model, sequence_length, batch_size, device, max_list, min_list\n",
    "):\n",
    "    min_list = [min_list[0], min_list[2], min_list[4]]\n",
    "    max_list = [max_list[0], max_list[2], max_list[4]]\n",
    "    generate_set = GenerateDataset(sequence_length, max_list, min_list)\n",
    "    loader = DataLoader(generate_set, batch_size=batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "    calibration_table = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            calibration_table.append(outputs)\n",
    "\n",
    "    calibration_table = np.vstack(calibration_table)\n",
    "    # 反归一化\n",
    "    min_value = min_list[2]\n",
    "    max_value = max_list[2]\n",
    "    calibration_table = calibration_table * (max_value - min_value) + min_value\n",
    "    calibration_table = pd.DataFrame(\n",
    "        {\n",
    "            \"cmd\": generate_set.input_data_ori[:, 0],\n",
    "            \"speed\": generate_set.input_data_ori[:, 1],\n",
    "            \"acc\": calibration_table[:, 0],\n",
    "        }\n",
    "    )\n",
    "    return calibration_table\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_size = 2\n",
    "    hidden_size = 8\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    batch_size = 20\n",
    "    learning_rate = 0.002\n",
    "    epochs = 100\n",
    "    sequence_length = 25\n",
    "    csv_file = \"/home/cyn/cs/NeuralNetwork_python/vehicle_model/record.csv\"\n",
    "\n",
    "    print_aligned_title(\"Config\")\n",
    "    print(\n",
    "        f\"input size:{input_size}, ouput size:{output_size}, hidden size:{hidden_size}, num layers:{num_layers}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"batch size:{batch_size}, learning rate:{learning_rate}, epochs:{epochs}, sequence length: {sequence_length}\\n\"\n",
    "    )\n",
    "\n",
    "    # 数据集\n",
    "    dataset = RecordDataset(csv_file, sequence_length)\n",
    "    # train_size = int(0.8 * len(dataset))  # 训练集占比 80%\n",
    "    # val_size = len(dataset) - train_size  # 验证集占比 20%\n",
    "    # train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "    # train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 模型\n",
    "    model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print_aligned_title(\"Using device: \" + str(device))\n",
    "    model.to(device)\n",
    "\n",
    "    # visualize_data(dataset)\n",
    "\n",
    "    # # 训练\n",
    "    final_loss = train(model, train_loader, criterion, optimizer, epochs)\n",
    "\n",
    "    # # 测试\n",
    "    # test_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "    # test(model, test_loader, criterion)\n",
    "\n",
    "    # 保存\n",
    "    current_time = time.strftime(\"%Y%m%d%H%M\", time.localtime())\n",
    "    save_dir = \"models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)  # 确保文件夹存在\n",
    "    if dataset.mode == 1:\n",
    "        save_name = f\"{current_time}_throttle_loss{final_loss}.pth\"\n",
    "    else:\n",
    "        save_name = f\"{current_time}_brake_loss{final_loss}.pth\"\n",
    "    save_path = os.path.join(save_dir, save_name)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"\\nModel saved successfully.\")\n",
    "\n",
    "    # # 生成标定表\n",
    "    # model_path = \"/home/cyn/cs/NeuralNetwork_python/vehicle_model/models/202403191535_throttle_loss0.0013729687514815936.pth\"\n",
    "    # model.load_state_dict(torch.load(model_path))\n",
    "    calibration_table = generate_calibration_table(\n",
    "        model, sequence_length, batch_size, device, dataset.min_list, dataset.max_list\n",
    "    )\n",
    "    calibration_table.to_csv(\"calibration_table.csv\", index=False)\n",
    "    print(\"Calibration table generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "  print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0011146910226761692\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "test_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, throttle_percentage\n",
      "1, brake_percentage\n",
      "2, steering_percentage\n",
      "3, speed_mps\n",
      "4, acceleration_current_point\n",
      "5, acceleration_next_point\n",
      "6, angular_velocity_vrf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "csv_file = \"/home/cyn/cs/NeuralNetwork_python/vehicle_model/record.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "data_samples = data[\n",
    "      [\n",
    "          \"throttle_percentage\",\n",
    "          \"brake_percentage\",\n",
    "          \"steering_percentage\",\n",
    "          \"speed_mps\",\n",
    "          \"acceleration_current_point\",\n",
    "          \"acceleration_next_point\",\n",
    "          \"angular_velocity_vrf\",\n",
    "      ]\n",
    "  ].head(1)\n",
    "# print(data_samples)\n",
    "for col_idx, col_name in enumerate(data_samples.columns):\n",
    "  print(f\"{col_idx}, {col_name}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 5.  0.]\n",
      " [10.  0.]\n",
      " ...\n",
      " [70. 10.]\n",
      " [75. 10.]\n",
      " [80. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cmd = np.arange(0, 81, 5)\n",
    "speed = np.arange(0, 10.1, 0.2)\n",
    "cmd, speed = np.meshgrid(cmd, speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
