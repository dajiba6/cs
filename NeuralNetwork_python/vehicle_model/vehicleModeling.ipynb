{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===================================== Config =====================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input size:3, ouput size:1, hidden size:8, num layers:2\n",
      "batch size:32, learning rate:0.001, epochs:100, sequence length: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "================================ Data processing =================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original samples: 307509\n",
      "Total filtered samples: 51267\n",
      "throttle left:41507, brake left:16049\n",
      "\n",
      "Epoch [1/100], Loss: 0.0013555199405909065\n",
      "Epoch [2/100], Loss: 0.0011879605554780605\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 209\u001b[0m\n\u001b[1;32m    206\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# 训练\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), save_path)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel saved successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[45], line 158\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m    156\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[1;32m    157\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m--> 158\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    160\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import logging\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "def print_aligned_title(title):\n",
    "    total_length = 80\n",
    "    title_length = len(title)\n",
    "    left_padding_length = (total_length - title_length) // 2\n",
    "    right_padding_length = total_length - title_length - left_padding_length\n",
    "    aligned_title = left_padding_length * \"=\" + \" \" + title + \" \" + right_padding_length * \"=\"\n",
    "    logging.info(aligned_title)\n",
    "\n",
    "# 数据集\n",
    "class RecordDataset(Dataset):\n",
    "    def __init__(self, csv_file, sequence_length):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.data_samples = self.data[\n",
    "            [\n",
    "                \"throttle_percentage\",\n",
    "                \"brake_percentage\",\n",
    "                \"speed_mps\",\n",
    "                \"steering_percentage\",\n",
    "                \"acceleration_current_point\",\n",
    "                \"acceleration_next_point\",\n",
    "                \"angular_velocity_vrf\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "        self.throttle_count = 0\n",
    "        self.brake_count = 0\n",
    "        big_group = []\n",
    "        index_tokeep = []\n",
    "\n",
    "        normalization_list = [[] for _ in range(len(self.data_samples.columns))]\n",
    "        # TODO:以指针形式选择数据，减少数据的复制移动\n",
    "        # 清理数据\n",
    "        for i in range(len(self.data_samples)):\n",
    "            steering_condition = (\n",
    "                abs(self.data_samples.at[i, \"steering_percentage\"]) <= 1\n",
    "            )\n",
    "            cmd_condition1 = (\n",
    "                self.data_samples.at[i, \"throttle_percentage\"] >= 1\n",
    "                or self.data_samples.at[i, \"brake_percentage\"] >= 1\n",
    "            )\n",
    "            cmd_condition2 = not (\n",
    "                self.data_samples.at[i, \"throttle_percentage\"] > 0\n",
    "                and self.data_samples.at[i, \"brake_percentage\"] > 0\n",
    "            )\n",
    "            speed_condition = self.data_samples.at[i, \"speed_mps\"] > 0\n",
    "\n",
    "            if (\n",
    "                steering_condition\n",
    "                and cmd_condition1\n",
    "                and cmd_condition2\n",
    "                and speed_condition\n",
    "            ):\n",
    "                index_tokeep.append(i)\n",
    "\n",
    "            else:\n",
    "                if len(index_tokeep) >= self.sequence_length:\n",
    "                    # 存储需要归一化的数据\n",
    "                    for idx in index_tokeep:\n",
    "                        row_data = self.data_samples.iloc[idx]\n",
    "                        for col_idx, col_name in enumerate(self.data_samples.columns):\n",
    "                            normalization_list[col_idx].append(row_data[col_name])\n",
    "\n",
    "                    small_group_data = [\n",
    "                        self.data_samples.iloc[idx] for idx in index_tokeep\n",
    "                    ]\n",
    "                    small_group_df = pd.DataFrame(small_group_data)\n",
    "                    big_group.append(small_group_df)\n",
    "                index_tokeep = []\n",
    "\n",
    "        # 最后一个数据\n",
    "        if len(index_tokeep) >= self.sequence_length:\n",
    "            for idx in index_tokeep:\n",
    "                row_data = self.data_samples.iloc[idx]\n",
    "                for col_idx, col_name in enumerate(self.data_samples.columns):\n",
    "                    normalization_list[col_idx].append(row_data[col_name])\n",
    "            small_group_data = [self.data_samples.iloc[idx] for idx in index_tokeep]\n",
    "            small_group_df = pd.DataFrame(small_group_data)\n",
    "            big_group.append(small_group_df)\n",
    "\n",
    "        # 归一化\n",
    "        normalization_list = np.array(normalization_list)\n",
    "        max_list = np.max(normalization_list, axis=1)\n",
    "        min_list = np.min(normalization_list, axis=1)\n",
    "        max_difference = max_list - min_list\n",
    "        for group in big_group:\n",
    "            for data_idx, row_data in group.iterrows():\n",
    "                if row_data[\"throttle_percentage\"] > 0:\n",
    "                    self.throttle_count += 1\n",
    "                else:\n",
    "                    self.brake_count += 1\n",
    "                group.loc[data_idx] = (row_data - min_list) / (max_list - min_list)\n",
    "\n",
    "        self.samples = []\n",
    "        for group in big_group:\n",
    "            for i in range(len(group)):\n",
    "                if i + sequence_length <= len(group):\n",
    "                    self.samples.append(group.iloc[i : i + sequence_length])\n",
    "\n",
    "        print_aligned_title(\"Data processing\")\n",
    "        print(f\"Total original samples: {len(self.data_samples)}\")\n",
    "        print(f\"Total filtered samples: {len(self.samples)}\")\n",
    "        print(f\"throttle left:{self.throttle_count}, brake left:{self.brake_count}\\n\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    # 选择用于训练的列\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(\n",
    "            self.samples[idx].iloc[:, :3].values, dtype=torch.float32\n",
    "        )\n",
    "        target = torch.tensor(\n",
    "            self.samples[idx].iloc[-1, -3:-2].values, dtype=torch.float32\n",
    "        )\n",
    "        return features, target\n",
    "\n",
    "\n",
    "# 网络\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 训练\n",
    "def train(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for features, target in train_loader:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "\n",
    "# 测试\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0.0\n",
    "        for features, target in test_loader:\n",
    "            features, target = features.to(device), target.to(device)\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, target)\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Test Loss: {total_loss / len(test_loader)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    input_size = 3\n",
    "    hidden_size = 8\n",
    "    num_layers = 2\n",
    "    output_size = 1\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.001\n",
    "    epochs = 100\n",
    "    sequence_length = 20\n",
    "    csv_file = \"/home/cyn/cs/NeuralNetwork_python/vehicle_model/record.csv\"\n",
    "    save_path = \"VehicleModel.pth\"\n",
    "    print_aligned_title(\"Config\")\n",
    "    print(f\"input size:{input_size}, ouput size:{output_size}, hidden size:{hidden_size}, num layers:{num_layers}\")\n",
    "    print(f\"batch size:{batch_size}, learning rate:{learning_rate}, epochs:{epochs}, sequence length: {sequence_length}\\n\")\n",
    "\n",
    "    # 数据集\n",
    "    dataset = RecordDataset(csv_file, sequence_length)\n",
    "    train_size = int(0.8 * len(dataset))  # 训练集占比 80%\n",
    "    val_size = len(dataset) - train_size  # 验证集占比 20%\n",
    "    train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # 模型\n",
    "    model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # 训练\n",
    "    train(model, train_loader, criterion, optimizer, epochs)\n",
    "\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(\"\\nModel saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0011146910226761692\n"
     ]
    }
   ],
   "source": [
    "# 测试\n",
    "test_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, throttle_percentage\n",
      "1, brake_percentage\n",
      "2, steering_percentage\n",
      "3, speed_mps\n",
      "4, acceleration_current_point\n",
      "5, acceleration_next_point\n",
      "6, angular_velocity_vrf\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "csv_file = \"/home/cyn/cs/NeuralNetwork_python/vehicle_model/record.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "data_samples = data[\n",
    "      [\n",
    "          \"throttle_percentage\",\n",
    "          \"brake_percentage\",\n",
    "          \"steering_percentage\",\n",
    "          \"speed_mps\",\n",
    "          \"acceleration_current_point\",\n",
    "          \"acceleration_next_point\",\n",
    "          \"angular_velocity_vrf\",\n",
    "      ]\n",
    "  ].head(1)\n",
    "# print(data_samples)\n",
    "for col_idx, col_name in enumerate(data_samples.columns):\n",
    "  print(f\"{col_idx}, {col_name}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.]\n",
      " [ 5.  0.]\n",
      " [10.  0.]\n",
      " ...\n",
      " [70. 10.]\n",
      " [75. 10.]\n",
      " [80. 10.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "cmd = np.arange(0, 81, 5)\n",
    "speed = np.arange(0, 10.1, 0.2)\n",
    "cmd, speed = np.meshgrid(cmd, speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 5]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
